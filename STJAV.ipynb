{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-markdown"
      },
      "source": [
        "# üîé Semantic Search Indexer\n",
        "**Build a neural search index from your CSV data.**\n",
        "\n",
        "1. **Initialize** the environment.\n",
        "2. **Upload** your `final_api_data.csv`.\n",
        "3. **Process** the data to generate embeddings.\n",
        "4. **Download** the resulting index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hgBZUCZqec93"
      },
      "outputs": [],
      "source": [
        "# @title 1. Initialize Environment\n",
        "# @markdown Run this cell first to install necessary libraries.\n",
        "\n",
        "%%capture\n",
        "!pip install sentence-transformers pandas numpy tqdm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Enable progress bars for pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "display(Markdown(\"‚úÖ **Libraries installed & Environment ready!**\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tvh4gkabek-g"
      },
      "outputs": [],
      "source": [
        "# @title 2. Upload Data\n",
        "# @markdown Upload your CSV file. The script will automatically look for a `.csv` file and rename it for processing.\n",
        "\n",
        "print(\"‚¨ÜÔ∏è Please upload your CSV file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "found_file = False\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.csv'):\n",
        "        os.rename(filename, 'final_api_data.csv')\n",
        "        display(Markdown(f\"‚úÖ **File loaded successfully:** `{filename}` renamed to `final_api_data.csv`\"))\n",
        "        found_file = True\n",
        "        break\n",
        "\n",
        "if not found_file:\n",
        "    display(Markdown(\"‚ùå **Error:** No CSV file found in upload. Please try again.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bOL_f9zCepk_"
      },
      "outputs": [],
      "source": [
        "# @title 3. Process & Index into LanceDB (Disk-based)\n",
        "# @markdown This creates a database on disk so you don't need huge RAM later.\n",
        "\n",
        "# --- INSTALL DATABASE ---\n",
        "!pip install lancedb\n",
        "\n",
        "import lancedb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
        "CSV_FILE = \"final_api_data.csv\"\n",
        "DB_FOLDER = \"jav_search_index\"\n",
        "TABLE_NAME = \"videos\"\n",
        "BATCH_SIZE = 50000  # Process 50k items at a time to save Colab RAM\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\.(mp4|wmv|avi|mkv|iso)\", \"\", text)\n",
        "    text = re.sub(r\"\\[.*?\\]\", \" \", text)\n",
        "    text = re.sub(r\"\\(.*?\\)\", \" \", text)\n",
        "    noise = [\"fhd\", \"hd\", \"sd\", \"1080p\", \"4k\", \"vr\", \"uncensored\", \"leaked\"]\n",
        "    pattern = r\"\\b(\" + \"|\".join(noise) + r\")\\b\"\n",
        "    text = re.sub(pattern, \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def create_rich_context(row):\n",
        "    title = clean_text(row.get(\"title\", \"\"))\n",
        "    jp_title = clean_text(row.get(\"jpTitle\", \"\"))\n",
        "    dvd_id = str(row.get(\"dvdId\", \"\")).strip()\n",
        "    \n",
        "    text_parts = []\n",
        "    if title: text_parts.append(title)\n",
        "    if jp_title and jp_title != title: text_parts.append(jp_title)\n",
        "    if dvd_id: text_parts.append(dvd_id) # Important for search matches\n",
        "    \n",
        "    # Prefix for e5 models\n",
        "    prefix = \"passage: \" if \"e5\" in MODEL_NAME else \"\"\n",
        "    return prefix + \" \".join(text_parts)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if os.path.exists(DB_FOLDER):\n",
        "    shutil.rmtree(DB_FOLDER) # Reset DB if exists\n",
        "os.makedirs(DB_FOLDER, exist_ok=True)\n",
        "\n",
        "# Initialize DB\n",
        "db = lancedb.connect(DB_FOLDER)\n",
        "\n",
        "print(f\"üß† Loading Model: {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"‚è≥ Reading CSV...\")\n",
        "# Read CSV in chunks to avoid memory overflow if file is huge\n",
        "# For simplicity here we read all text but embed in batches\n",
        "df_full = pd.read_csv(CSV_FILE)\n",
        "df_full = df_full.fillna(\"\") # Fill NAs\n",
        "\n",
        "print(\"üßπ Preparing text...\")\n",
        "df_full[\"search_text\"] = df_full.apply(create_rich_context, axis=1)\n",
        "df_full = df_full[df_full[\"search_text\"].str.len() > 5] # Filter garbage\n",
        "\n",
        "print(f\"üöÄ Indexing {len(df_full)} items into Vector DB...\")\n",
        "\n",
        "data_buffer = []\n",
        "total_batches = (len(df_full) // BATCH_SIZE) + 1\n",
        "\n",
        "# Create Table (using first item to infer schema)\n",
        "# We need to structure data explicitly for LanceDB\n",
        "# [vector, dvdId, title, jpTitle, releaseDate, image, generated_url]\n",
        "\n",
        "for i in tqdm(range(0, len(df_full), BATCH_SIZE), desc=\"Processing Batches\"):\n",
        "    batch = df_full.iloc[i : i + BATCH_SIZE].copy()\n",
        "    \n",
        "    # Encode\n",
        "    sentences = batch[\"search_text\"].tolist()\n",
        "    embeddings = model.encode(sentences, normalize_embeddings=True, show_progress_bar=False)\n",
        "    \n",
        "    # Prepare batch for DB\n",
        "    chunk_data = []\n",
        "    for idx, row in enumerate(batch.to_dict(\"records\")):\n",
        "        chunk_data.append({\n",
        "            \"vector\": embeddings[idx],\n",
        "            \"dvdId\": str(row[\"dvdId\"]),\n",
        "            \"title\": str(row[\"title\"]),\n",
        "            \"jpTitle\": str(row[\"jpTitle\"]),\n",
        "            \"releaseDate\": str(row[\"releaseDate\"]),\n",
        "            \"image\": str(row[\"image\"]),\n",
        "            \"generated_url\": str(row[\"generated_url\"])\n",
        "        })\n",
        "    \n",
        "    # Add to DB\n",
        "    if i == 0:\n",
        "        table = db.create_table(TABLE_NAME, data=chunk_data, mode=\"overwrite\")\n",
        "    else:\n",
        "        table.add(chunk_data)\n",
        "\n",
        "print(f\"‚úÖ Indexing complete. Total items in DB: {len(table)}\")\n",
        "\n",
        "# Create an IVF-PQ index for speed on large datasets (Optional but recommended for >100k items)\n",
        "if len(table) > 10000:\n",
        "    print(\"‚öôÔ∏è Building optimized index (IVF-PQ)... this makes search fast on laptops.\")\n",
        "    table.create_index(metric=\"cosine\", vector_column_name=\"vector\")\n",
        "    print(\"‚úÖ Index built.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SPyZKLD1exdj"
      },
      "outputs": [],
      "source": [
        "# @title 4. Compress & Download (With Progress Bar)\n",
        "# @markdown Zips the database folder manually so you can see progress, then triggers download.\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "SOURCE_FOLDER = \"jav_search_index\"\n",
        "OUTPUT_FILENAME = \"jav_search_index.zip\"\n",
        "\n",
        "def zipdir_with_progress(path, ziph):\n",
        "    # 1. Count total files first for the progress bar\n",
        "    print(\"üìä Calculating total files to zip...\")\n",
        "    total_files = sum([len(files) for r, d, files in os.walk(path)])\n",
        "    print(f\"   Found {total_files} files.\")\n",
        "    \n",
        "    # 2. Zip with progress bar\n",
        "    with tqdm(total=total_files, unit=\"file\", desc=\"üì¶ Zipping\") as pbar:\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                # Absolute path\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Path inside zip\n",
        "                arcname = os.path.relpath(file_path, os.path.join(path, '..'))\n",
        "                \n",
        "                ziph.write(file_path, arcname)\n",
        "                pbar.update(1)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if os.path.exists(SOURCE_FOLDER):\n",
        "    print(f\"üöÄ Starting compression of '{SOURCE_FOLDER}'...\")\n",
        "    \n",
        "    # Create Zip File\n",
        "    with zipfile.ZipFile(OUTPUT_FILENAME, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipdir_with_progress(SOURCE_FOLDER, zipf)\n",
        "    \n",
        "    # Check size\n",
        "    size_mb = os.path.getsize(OUTPUT_FILENAME) / (1024 * 1024)\n",
        "    print(f\"‚úÖ Compression Complete! File size: {size_mb:.2f} MB\")\n",
        "    \n",
        "    print(\"‚¨áÔ∏è Triggering Download (Check your browser downloads)...\")\n",
        "    files.download(OUTPUT_FILENAME)\n",
        "else:\n",
        "    print(f\"‚ùå Error: Folder '{SOURCE_FOLDER}' not found. Did Step 3 finish?\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
