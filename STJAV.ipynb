{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Semantic Search Indexer\n",
        "**Build a neural search index from your CSV data.**\n",
        "\n",
        "1. **Initialize** the environment.\n",
        "2. **Upload** your `final_api_data.csv`.\n",
        "3. **Process** the data to generate embeddings.\n",
        "4. **Download** the resulting index."
      ],
      "metadata": {
        "id": "header-markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgBZUCZqec93",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 1. Initialize Environment\n",
        "# @markdown Run this cell first to install necessary libraries.\n",
        "\n",
        "%%capture\n",
        "!pip install sentence-transformers pandas numpy tqdm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Enable progress bars for pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "display(Markdown(\"‚úÖ **Libraries installed & Environment ready!**\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Upload Data\n",
        "# @markdown Upload your CSV file. The script will automatically look for a `.csv` file and rename it for processing.\n",
        "\n",
        "print(\"‚¨ÜÔ∏è Please upload your CSV file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "found_file = False\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.csv'):\n",
        "        os.rename(filename, 'final_api_data.csv')\n",
        "        display(Markdown(f\"‚úÖ **File loaded successfully:** `{filename}` renamed to `final_api_data.csv`\"))\n",
        "        found_file = True\n",
        "        break\n",
        "\n",
        "if not found_file:\n",
        "    display(Markdown(\"‚ùå **Error:** No CSV file found in upload. Please try again.\"))"
      ],
      "metadata": {
        "id": "tvh4gkabek-g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Process & Embed\n",
        "# @markdown This step cleans the text, formats it for the model, and generates vectors.\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-large\" # @param [\"intfloat/multilingual-e5-large\", \"sentence-transformers/all-MiniLM-L6-v2\", \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"]\n",
        "CSV_FILE = \"final_api_data.csv\"\n",
        "EMBEDDING_FILE = \"search_embeddings.npy\"\n",
        "METADATA_FILE = \"search_metadata.pkl\"\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    # Remove extensions\n",
        "    text = re.sub(r\"\\.(mp4|wmv|avi|mkv|iso)\", \"\", text)\n",
        "    # Remove brackets\n",
        "    text = re.sub(r\"\\[.*?\\]\", \" \", text)\n",
        "    text = re.sub(r\"\\(.*?\\)\", \" \", text)\n",
        "    # Remove common noise keywords\n",
        "    noise = [\"fhd\", \"hd\", \"sd\", \"1080p\", \"4k\", \"vr\", \"uncensored\", \"leaked\"]\n",
        "    pattern = r\"\\b(\" + \"|\".join(noise) + r\")\\b\"\n",
        "    text = re.sub(pattern, \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def create_rich_context(row):\n",
        "    # Prepare parts\n",
        "    title = clean_text(row.get(\"title\", \"\"))\n",
        "    jp_title = clean_text(row.get(\"jpTitle\", \"\"))\n",
        "    tags = clean_text(row.get(\"tags\", \"\")) if \"tags\" in row else \"\"\n",
        "    actress = clean_text(row.get(\"actress\", \"\")) if \"actress\" in row else \"\"\n",
        "    maker = clean_text(row.get(\"maker\", \"\")) if \"maker\" in row else \"\"\n",
        "\n",
        "    text_parts = []\n",
        "    if title: text_parts.append(title)\n",
        "    if jp_title and jp_title != title: text_parts.append(jp_title)\n",
        "    if actress: text_parts.append(f\"starring {actress}\")\n",
        "    if maker: text_parts.append(f\"studio {maker}\")\n",
        "    if tags: text_parts.append(f\"genres {tags}\")\n",
        "\n",
        "    # E5 models require \"passage: \" prefix for documents\n",
        "    prefix = \"passage: \" if \"e5\" in MODEL_NAME else \"\"\n",
        "    return prefix + \" \".join(text_parts)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if not os.path.exists(CSV_FILE):\n",
        "    print(\"‚ùå CSV file not found. Please run Step 2 first.\")\n",
        "else:\n",
        "    print(\"‚è≥ Reading CSV...\")\n",
        "    df = pd.read_csv(CSV_FILE)\n",
        "\n",
        "    print(\"üßπ Cleaning data & creating rich context...\")\n",
        "    # Using progress_apply for visualization\n",
        "    df[\"search_text\"] = df.progress_apply(create_rich_context, axis=1)\n",
        "\n",
        "    # Filter empty or too short rows\n",
        "    initial_len = len(df)\n",
        "    df = df[df[\"search_text\"].str.len() > 10]\n",
        "    print(f\"   üìâ Filtered: {initial_len} -> {len(df)} items (removed empty/short)\")\n",
        "\n",
        "    print(f\"üß† Loading Model: {MODEL_NAME}...\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"   üî• Computing on: {device.upper()}\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "    print(f\"üöÄ Generating Embeddings...\")\n",
        "    sentences = df[\"search_text\"].tolist()\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = model.encode(\n",
        "        sentences,\n",
        "        show_progress_bar=True,\n",
        "        batch_size=32,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "    print(\"üíæ Saving temporary files...\")\n",
        "    np.save(EMBEDDING_FILE, embeddings)\n",
        "    df.to_pickle(METADATA_FILE)\n",
        "\n",
        "    display(Markdown(\"‚úÖ **Indexing Complete! Proceed to Step 4.**\"))"
      ],
      "metadata": {
        "id": "bOL_f9zCepk_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Download Results\n",
        "# @markdown Zips the embeddings and metadata, then triggers a download.\n",
        "\n",
        "OUTPUT_ZIP = \"search_engine_data\"\n",
        "\n",
        "if os.path.exists(\"search_embeddings.npy\"):\n",
        "    print(\"üì¶ Zipping files...\")\n",
        "    shutil.make_archive(OUTPUT_ZIP, 'zip', '.', '.')\n",
        "\n",
        "    print(\"‚¨áÔ∏è Downloading...\")\n",
        "    files.download(f'{OUTPUT_ZIP}.zip')\n",
        "else:\n",
        "    display(Markdown(\"‚ùå **Files not found.** Please run Step 3 successfully first.\"))"
      ],
      "metadata": {
        "id": "SPyZKLD1exdj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
