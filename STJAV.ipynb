{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-markdown"
      },
      "source": [
        "# üîé Semantic Search Indexer (With Google Drive Support)\n",
        "**Build a neural search index from your CSV data.**\n",
        "\n",
        "1. **Initialize** (Optionally connect Google Drive).\n",
        "2. **Load Data** (Checks Drive folder `JSearchEngine` first, then falls back to upload).\n",
        "3. **Process** the data to generate embeddings.\n",
        "4. **Export** (Saves to Drive if connected, otherwise downloads via browser)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hgBZUCZqec93"
      },
      "outputs": [],
      "source": [
        "# @title 1. Initialize Environment & Connect Drive\n",
        "# @markdown Install libraries and optionally mount Google Drive.\n",
        "CONNECT_GOOGLE_DRIVE = True # @param {type:\"boolean\"}\n",
        "\n",
        "%%capture\n",
        "!pip install sentence-transformers pandas numpy tqdm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Enable progress bars for pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/JSearchEngine\"\n",
        "DRIVE_MOUNTED = False\n",
        "\n",
        "if CONNECT_GOOGLE_DRIVE:\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "    \n",
        "    if os.path.exists('/content/drive'):\n",
        "        DRIVE_MOUNTED = True\n",
        "        os.makedirs(DRIVE_BASE, exist_ok=True)\n",
        "        display(Markdown(f\"‚úÖ **Google Drive Mounted!** Working folder: `{DRIVE_BASE}`\"))\n",
        "    else:\n",
        "        display(Markdown(\"‚ö†Ô∏è **Drive mount failed.** Proceeding with local storage.\"))\n",
        "else:\n",
        "    display(Markdown(\"‚úÖ **Libraries installed.** (Google Drive disabled).\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tvh4gkabek-g"
      },
      "outputs": [],
      "source": [
        "# @title 2. Load Data\n",
        "# @markdown The script looks for a CSV file in `JSearchEngine` on Drive. If not found, it asks for an upload.\n",
        "\n",
        "TARGET_FILENAME = 'final_api_data.csv'\n",
        "found_file = False\n",
        "\n",
        "# 1. Check Google Drive First\n",
        "if DRIVE_MOUNTED:\n",
        "    drive_file_path = os.path.join(DRIVE_BASE, TARGET_FILENAME)\n",
        "    \n",
        "    # Check if specific file exists, or look for any CSV\n",
        "    if os.path.exists(drive_file_path):\n",
        "        print(f\"üìÇ Found file in Drive: {drive_file_path}\")\n",
        "        shutil.copy(drive_file_path, TARGET_FILENAME)\n",
        "        found_file = True\n",
        "    else:\n",
        "        # Look for any CSV in the JSearchEngine folder\n",
        "        csv_files = [f for f in os.listdir(DRIVE_BASE) if f.endswith('.csv')]\n",
        "        if csv_files:\n",
        "            print(f\"üìÇ Found CSV in Drive: {csv_files[0]}\")\n",
        "            shutil.copy(os.path.join(DRIVE_BASE, csv_files[0]), TARGET_FILENAME)\n",
        "            found_file = True\n",
        "\n",
        "# 2. Fallback to Upload\n",
        "if found_file:\n",
        "    display(Markdown(f\"‚úÖ **Data Loaded successfully from Drive.**\"))\n",
        "else:\n",
        "    print(\"‚¨áÔ∏è No CSV found in Drive folder. Please upload manually:\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.endswith('.csv'):\n",
        "            os.rename(filename, TARGET_FILENAME)\n",
        "            display(Markdown(f\"‚úÖ **File uploaded:** `{filename}` renamed to `{TARGET_FILENAME}`\"))\n",
        "            found_file = True\n",
        "            break\n",
        "    \n",
        "    if not found_file:\n",
        "        display(Markdown(\"‚ùå **Error:** No CSV file provided.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bOL_f9zCepk_"
      },
      "outputs": [],
      "source": [
        "# @title 3. Process & Index into LanceDB\n",
        "# @markdown Indexes the data locally first for speed.\n",
        "\n",
        "# --- INSTALL DATABASE ---\n",
        "!pip install lancedb\n",
        "\n",
        "import lancedb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-large\" \n",
        "CSV_FILE = \"final_api_data.csv\"\n",
        "DB_FOLDER = \"jav_search_index\"\n",
        "TABLE_NAME = \"videos\"\n",
        "BATCH_SIZE = 50000 \n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\.(mp4|wmv|avi|mkv|iso)\", \"\", text)\n",
        "    text = re.sub(r\"\\[.*?\\]\", \" \", text)\n",
        "    text = re.sub(r\"\\(.*?\\)\", \" \", text)\n",
        "    noise = [\"fhd\", \"hd\", \"sd\", \"1080p\", \"4k\", \"vr\", \"uncensored\", \"leaked\"]\n",
        "    pattern = r\"\\b(\" + \"|\".join(noise) + r\")\\b\"\n",
        "    text = re.sub(pattern, \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def create_rich_context(row):\n",
        "    title = clean_text(row.get(\"title\", \"\"))\n",
        "    jp_title = clean_text(row.get(\"jpTitle\", \"\"))\n",
        "    dvd_id = str(row.get(\"dvdId\", \"\")).strip()\n",
        "    actresses = str(row.get(\"actress_names\", \"\")).replace(\",\", \" \")\n",
        "    \n",
        "    text_parts = []\n",
        "    if actresses: text_parts.append(f\"Starring: {actresses}.\")\n",
        "    if title: text_parts.append(title)\n",
        "    if jp_title and jp_title != title: text_parts.append(jp_title)\n",
        "    if dvd_id: text_parts.append(dvd_id)\n",
        "    \n",
        "    prefix = \"passage: \" if \"e5\" in MODEL_NAME else \"\"\n",
        "    return prefix + \" \".join(text_parts)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if os.path.exists(DB_FOLDER):\n",
        "    shutil.rmtree(DB_FOLDER) \n",
        "os.makedirs(DB_FOLDER, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(CSV_FILE):\n",
        "    print(\"‚ùå CSV File not found! Run Step 2.\")\n",
        "else:\n",
        "    db = lancedb.connect(DB_FOLDER)\n",
        "\n",
        "    print(f\"üß† Loading Model: {MODEL_NAME}...\")\n",
        "    model = SentenceTransformer(MODEL_NAME)\n",
        "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(\"‚è≥ Reading CSV...\")\n",
        "    df_full = pd.read_csv(CSV_FILE)\n",
        "    df_full = df_full.fillna(\"\")\n",
        "\n",
        "    print(\"üßπ Preparing text...\")\n",
        "    df_full[\"search_text\"] = df_full.apply(create_rich_context, axis=1)\n",
        "    df_full = df_full[df_full[\"search_text\"].str.len() > 5]\n",
        "\n",
        "    print(f\"üöÄ Indexing {len(df_full)} items into Vector DB...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(df_full), BATCH_SIZE), desc=\"Processing Batches\"):\n",
        "        batch = df_full.iloc[i : i + BATCH_SIZE].copy()\n",
        "        sentences = batch[\"search_text\"].tolist()\n",
        "        embeddings = model.encode(sentences, normalize_embeddings=True, show_progress_bar=False)\n",
        "        \n",
        "        chunk_data = []\n",
        "        for idx, row in enumerate(batch.to_dict(\"records\")):\n",
        "            chunk_data.append({\n",
        "                \"vector\": embeddings[idx],\n",
        "                \"dvdId\": str(row[\"dvdId\"]),\n",
        "                \"title\": str(row[\"title\"]),\n",
        "                \"jpTitle\": str(row[\"jpTitle\"]),\n",
        "                \"actress_names\": str(row[\"actress_names\"]),\n",
        "                \"releaseDate\": str(row[\"releaseDate\"]),\n",
        "                \"image\": str(row[\"image\"]),\n",
        "                \"generated_url\": str(row[\"generated_url\"])\n",
        "            })\n",
        "        \n",
        "        if i == 0:\n",
        "            table = db.create_table(TABLE_NAME, data=chunk_data, mode=\"overwrite\")\n",
        "        else:\n",
        "            table.add(chunk_data)\n",
        "\n",
        "    print(f\"‚úÖ Indexing complete. Total items in DB: {len(table)}\")\n",
        "\n",
        "    if len(table) > 10000:\n",
        "        print(\"‚öôÔ∏è Building optimized index (IVF-PQ)...\")\n",
        "        table.create_index(metric=\"cosine\", vector_column_name=\"vector\")\n",
        "        print(\"‚úÖ Index built.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SPyZKLD1exdj"
      },
      "outputs": [],
      "source": [
        "# @title 4. Compress & Export to Drive\n",
        "# @markdown If Drive is connected, the zip file is saved to `JSearchEngine`. Otherwise, it downloads to your browser.\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "SOURCE_FOLDER = \"jav_search_index\"\n",
        "OUTPUT_FILENAME = \"jav_search_index.zip\"\n",
        "\n",
        "def zipdir_with_progress(path, ziph):\n",
        "    total_files = sum([len(files) for r, d, files in os.walk(path)])\n",
        "    print(f\"üìä Total files: {total_files}\")\n",
        "    with tqdm(total=total_files, unit=\"file\", desc=\"üì¶ Zipping\") as pbar:\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, os.path.join(path, '..'))\n",
        "                ziph.write(file_path, arcname)\n",
        "                pbar.update(1)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if os.path.exists(SOURCE_FOLDER):\n",
        "    print(f\"üöÄ Zipping database...\")\n",
        "    with zipfile.ZipFile(OUTPUT_FILENAME, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipdir_with_progress(SOURCE_FOLDER, zipf)\n",
        "    \n",
        "    size_mb = os.path.getsize(OUTPUT_FILENAME) / (1024 * 1024)\n",
        "    print(f\"‚úÖ Compression Complete! Size: {size_mb:.2f} MB\")\n",
        "\n",
        "    # Check where to send it\n",
        "    if DRIVE_MOUNTED and os.path.exists(DRIVE_BASE):\n",
        "        dest_path = os.path.join(DRIVE_BASE, OUTPUT_FILENAME)\n",
        "        print(f\"‚òÅÔ∏è Copying to Google Drive ({DRIVE_BASE})...\")\n",
        "        shutil.copy(OUTPUT_FILENAME, dest_path)\n",
        "        display(Markdown(f\"‚úÖ **Saved to Drive:** `{dest_path}`\"))\n",
        "    else:\n",
        "        print(\"‚¨áÔ∏è Triggering Browser Download...\")\n",
        "        files.download(OUTPUT_FILENAME)\n",
        "else:\n",
        "    print(f\"‚ùå Error: Folder '{SOURCE_FOLDER}' not found. Did Step 3 finish?\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
